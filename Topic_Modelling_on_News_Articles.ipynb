{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshayAI007/Topic-modelling-of-news-article-/blob/main/Topic_Modelling_on_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Topic modeling of news article\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised ML\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exponential growth of digital content has led to an overwhelming influx of news articles across various domains. Manual categorization and analysis of this vast amount of information are labor-intensive and time-consuming. Unsupervised topic modeling offers a solution by automating the process of identifying key themes within the articles, enabling efficient content organization and information retrieval.**Unsupervised machine learning** techniques have gained significant traction in the field of natural language processing (NLP) due to their ability to extract valuable insights from unstructured text data. This project focuses on applying clustering algorithms, particularly **Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA)**, to perform topic modeling on a diverse collection of news articles. The goal is to uncover hidden thematic structures within the articles and categorize them into coherent topics."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to leverage unsupervised machine learning techniques to perform topic modeling on a collection of news articles from BBC using clustering algorithms such as Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA). The goal is to extract meaningful topics from the articles, allowing for efficient content organization, information retrieval, and trend analysis.The dataset Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n",
        "Natural Classes: 5 (business, entertainment, politics, sport, tech)"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# importing CountVectorizer for feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Importing data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# importing tqdm and display modules for progress meters/bars\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "# importing wordcloud to represent topics wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Model selection modules\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import ast\n",
        "\n",
        "# importing data visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# importing mlab for implementing MATLAB functions\n",
        "import matplotlib.mlab as mlab\n",
        "\n",
        "# importing statistics module\n",
        "import scipy.stats as stats\n",
        "\n",
        "# importing decomposition modules\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# importing Natural Language Toolkit and other NLP modules\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# Importing warnings library. The warnings module handles warnings in Python.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR",
        "outputId": "a76a8fcc-7985-4a1a-849e-5705e181e313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "l3VgjBlEUyNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ff5576-5591-4261-ece9-1d7c9eb318d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.   For loading the dataset, the **glob and re** libraries were used.While reading the data, each paragraph was read into a different row with the column index being the title.\n",
        "\n",
        " 2. This data was organised into three features - the first column named \"Title\", the rest of the article in \"Description\" and the category assigned to the article in \"Category\" column\n",
        " 1.   Articles from each category were stored in separate DataFrames (for any furture usage), and all were concatenated to form a single final DataFrame\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KppPPfYbBEml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The variable \"directory\" holds the address of text files stored in drive\n",
        "directory = '/content/drive/MyDrive/Projects/Topic_Modeling_on_news_articles/bbc/bbc'\n",
        "\n",
        "# All 5 sub-categories provided\n",
        "subdirs = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "\n",
        "# Create dataframe for gathering the articles\n",
        "bbc = pd.DataFrame()\n",
        "\n",
        "# Iterate over sub-directories to access the text files\n",
        "for subdir in subdirs:\n",
        "\n",
        "  # address to the subdirectory\n",
        "  dir = directory + '/' + subdir\n",
        "\n",
        "  # Iterate over all the text files present in a sub-directory\n",
        "  for filename in os.listdir(dir):\n",
        "\n",
        "    # Get file address\n",
        "    filepath = os.path.join(dir, filename)\n",
        "\n",
        "    # Traversing over text files and storing the articles into the dataframe\n",
        "    try:\n",
        "      data = open(filepath,'r').read()\n",
        "\n",
        "      # escape characters to be ignored in the text\n",
        "      escape = ['\\n']\n",
        "\n",
        "      # removing escape characters from text\n",
        "      for elem in escape:\n",
        "        data = data.replace(elem, ' ')\n",
        "\n",
        "      # Storing article to the dataframe\n",
        "      dict1 = {'Filename': filename.split('.')[0], 'Contents': data.lower(), 'Category':subdir}\n",
        "      bbc = bbc.append(dict1, ignore_index=True, verify_integrity = True)\n",
        "\n",
        "    # Ignore exception, if any\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "UzZguZWlBDSK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc.shape"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R",
        "outputId": "42d3fed5-e402-4b6a-dbfa-1b5069ad6507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset contains a set of news articles for each major segment consisting of business, entertainment, politics, sports and technology. There are over 2000 news article available in these categories.**"
      ],
      "metadata": {
        "id": "B2hXTyGlfnU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc.sample(10)\n"
      ],
      "metadata": {
        "id": "gE2iIReUkOhT",
        "outputId": "dc6180ea-343c-45fd-b7a5-756aaa874b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4089d777b0d9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5856\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5858\u001b[0;31m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5859\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/sample.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid weights: weights sum to zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     )\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset consists of 4 columns:**\n",
        "\n",
        "\n",
        "1.   **Index** : Entry number\n",
        "\n",
        "1.   **Filname** : Destination File Name/ Number\n",
        "2.   **Contents** : Complete transcript of the article, the complete textual data\n",
        "\n",
        "2.   **Category** : Article topic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eEuPQllIfwgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bbc.info()"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df = bbc.copy()                                                               ## First creating a deep copy\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "3IE0L9KKuDSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check for duplicates cause having duplicates will result in inconsistencies\n",
        "df.duplicated(subset = ['Contents']).sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the duplicate values\n",
        "df.drop_duplicates(subset = ['Contents'],inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "bhMug3K0ugYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset contains total 99 duplicate rows and does not contain any missing values**"
      ],
      "metadata": {
        "id": "5elITFouvGi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset does not contain any missing values**"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's add a column which shows us the number of words used in each article**"
      ],
      "metadata": {
        "id": "pVXaCxZ7wkrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Contents_len'] = df['Contents'].str.len()\n"
      ],
      "metadata": {
        "id": "TVZo989KiEJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "sns.distplot(df['Contents_len']).set_title('News length distribution');"
      ],
      "metadata": {
        "id": "F7OnITHpiPJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see here that most of the contents length lie between 0-5000 but a few goes as high as 25000.**"
      ],
      "metadata": {
        "id": "U9DYj6chiWjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "None"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "#Dist plot for each category\n",
        "sns.displot(df, x=\"Contents_len\", hue=\"Category\", kind=\"kde\",height=7,aspect =1 )"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we can see the content length distribution for each categories.**"
      ],
      "metadata": {
        "id": "R1UvXoMpire6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "t3XG4dCcisz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the number of Article per categories:\n",
        "cat_count =df.groupby(['Category'],)['Category'].count()\n"
      ],
      "metadata": {
        "id": "B93JlItQi1zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "cat_count.plot(kind ='bar', grid =True)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Count per Category\")"
      ],
      "metadata": {
        "id": "8zGGCAOsi-X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bar chart gives us an apt idea about the number of articles in each category**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see here the highest number of content is for the business and the spors category.**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No missing values hence not needed**"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not needed**"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not needed**"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Let's Check the textual content of the Data:"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = df.reset_index()\n",
        "content = content['Contents']\n",
        "content"
      ],
      "metadata": {
        "id": "MdZOuJXXjmrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space,\n",
        "    # which in effect deletes the punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Contents'] = df['Contents'].apply(remove_punctuation)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "mPLrJtbokh2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Regular expression pattern to match URLs\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "\n",
        "    # Replace URLs with an empty string\n",
        "    cleaned_text = re.sub(url_pattern, '', text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "df['Contents']=df['Contents'].apply(remove_urls)\n",
        "\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountVectorizer(Top Words):**\n",
        "\n",
        "In order to use textual data for predictive modeling, the text must be parsed to remove certain words â€“ this process is called **Tokenization.**\n",
        "\n",
        "These words then need to be encoded as integers, or floating-point values, such that they can be used as inputs in machine learning algorithms. This process is called **Feature Extraction (or Vectorization)**."
      ],
      "metadata": {
        "id": "6mz-XhgvxJBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Function to extract top n words with highest frequency\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    The function returns a tuple of the top n words in a sample and their\n",
        "    accompanying counts, given a CountVectorizer object and text sample as inputs\n",
        "    '''\n",
        "    # encoding the document using countvectorizer object\n",
        "    vectorized_content = count_vectorizer.fit_transform(text_data.values)\n",
        "    vectorized_total = np.sum(vectorized_content, axis=0)\n",
        "\n",
        "    # extracting specifics for words\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
        "\n",
        "    # creating a vector matrix for words\n",
        "    word_vectors = np.zeros((n_top_words, vectorized_content.shape[1]))\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    # display Vector matrix\n",
        "    print(word_vectors)\n",
        "\n",
        "    # collect the words\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for\n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Stopword removal**"
      ],
      "metadata": {
        "id": "O1VDhrQ605Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# dowloading nltk stopwords module\n",
        "nltk.download('stopwords')\n",
        "# extracting all stopwords for english language\n",
        "stop = nltk.corpus.stopwords.words('english')\n",
        "stop[0:10]"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.Vectorization**"
      ],
      "metadata": {
        "id": "u_x1d6th1FuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating vectorizer object\n",
        "count_vectorizer = CountVectorizer(stop_words=stop)\n",
        "\n",
        "# calling the function to get words and their counts\n",
        "words, word_values = get_top_n_words(n_top_words=25,\n",
        "                                     count_vectorizer=count_vectorizer,\n",
        "                                     text_data=content)\n",
        "\n",
        "# display top 25 words using bar plot\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(range(len(words)), word_values, edgecolor='red')\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation='vertical')\n",
        "ax.set_title('Top words in new articles dataset (excluding stop words)')\n",
        "ax.set_xlabel('Words')\n",
        "ax.set_ylabel('Number of occurences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8HqNbVOexeqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Text Normalization**"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING AND LEMMATIZING THE DATA**\n",
        "\n",
        "**Stemming**: is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
        "\n",
        "**Lemmatization**: This algorithm collects all inflected forms of a word in order to break them down to their root dictionary form or lemma. Words are broken down into a part of speech (the categories of word types) by way of the rules of grammar."
      ],
      "metadata": {
        "id": "QIUnFfiqyZ5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# making a lemmatizer object\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "\n",
        "# lemmatizing an article to see what snowball lemmatizer returns\n",
        "for rows in content:\n",
        " print(rows)\n",
        " test = [sno.stem(words) for words in rows.split(' ')]\n",
        " print(test)\n",
        " break\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we use a Lemmatization technique we can see that it has worked but some words dont make any sense now such as 'associated' is 'associ' and 'aggregate' to 'aggreg'.Hence the meaning of the words are lost in the process so we wont be using this method.**"
      ],
      "metadata": {
        "id": "csRL8fbXyu1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization is a technique that converts the text content to numerical feature vectors. Bag of Words takes a document from a corpus and converts it into a numeric vector by mapping each document word to a feature vector for the machine learning model.**"
      ],
      "metadata": {
        "id": "_UwQRmxjzBRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# creating a countvectorizer object\n",
        "count_vectorizer = CountVectorizer(stop_words = stop, max_features = 4000)\n",
        "\n",
        "# text before vectorization\n",
        "text_sample = content\n",
        "print('Content after removing Stopwords and Punctuations: \\n{}'.format(text_sample[23]))\n",
        "\n",
        "# encode the textual content\n",
        "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
        "\n",
        "# text after vectorization\n",
        "print('Vectorization: \\n{}'.format(document_term_matrix[23]))\n",
        "\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model - Latent Semantic Analysis (LSA):**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Latent Semantic Analysis (LSA)** is a method that allows us to extract topics from documents by converting their text into word-topic and document-topic matrices. The procedure for LSA is relatively straightforward: Convert the text corpus into a document-term matrix. Implement truncated singular value decomposition."
      ],
      "metadata": {
        "id": "HtS-kKOL-7Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n"
      ],
      "metadata": {
        "id": "zKC8XMO8_Ac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# SVD represent documents and terms in vectors\n",
        "svd_model = TruncatedSVD(n_components=5, algorithm='randomized', n_iter=100, random_state=23)\n",
        "# Fit the Algorithm\n",
        "svd_model.fit(document_term_matrix)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "terms = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:40]\n",
        "    print(\" \")\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    print(\" \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0])"
      ],
      "metadata": {
        "id": "qf6gLFxL_StP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above the LSA model does group the words into topics but the words are heavily mismatched for just 40 words,for example:\n",
        "\n",
        "\n",
        "\n",
        "1.   Topic 0 has definded 'music' , 'government' and 'game' in the same cluster\n",
        "\n",
        "1.  Topic 1 has definded 'songs' and 'mobile' in the same cluster\n",
        "\n",
        "2.  Topic 2 has definded 'labour' and 'party' in the same cluster\n",
        "\n",
        "2.   Topic 3 has definded 'nadal' and 'election' in the same cluster\n",
        "\n",
        "1.  Topic 4 is comparatively clustered better because most of the words belongs to technology but even so there are words like 'government' , 'wage' , 'urban' which are not accurate.\n",
        "\n",
        "\n",
        "# ***Since the LSA model performs very poorly we won't be using this as our clustering algorithm.***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9TSFDtlh_j-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model -Latent Dirichlet Allocation (LDA):**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Latent Dirichlet Allocation (LDA)** algorithm is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. LDA is most commonly used to discover a user-specified number of topics shared by documents within a text corpus.\n",
        "\n",
        "An advantage of the LDA technique is that one does not have to know in advance what the topics will look like. By tuning the LDA parameters to fit different dataset shapes, one can explore topic formation and resulting document clusters.\n",
        "\n",
        "The goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.\n",
        "\n",
        "\n",
        "> We will be using pyLDAvis which allows a better visualization.\n",
        "\n",
        ">We will be using T-Sne for lowering down the dimensions of our feature-space"
      ],
      "metadata": {
        "id": "TEUI7zsEA9Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Evaluation:***\n",
        "\n",
        "For Information retreival task like Topic modelling, there two metrics to judge the model performance.\n",
        "\n",
        ">**Perplexity:** is a mesure of model complication by trained model when exposed to unseen documents. For good model perplexity score should be less. Higher the score model not generlise well.\n",
        "\n",
        ">**Coherence score:** is measure of sum of sementic similarity between most occurence words for every topic."
      ],
      "metadata": {
        "id": "RQf_SaGWsKxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "\n",
        "# Create an LDA model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Define hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_components': [5, 10, 15],          # Number of topics\n",
        "    'learning_method': ['online', 'batch'],  # Learning method\n",
        "    'learning_decay': [0.5, 0.7, 0.9]        # Learning rate decay\n",
        "}\n",
        "\n",
        "# Perform grid search using 5-fold cross-validation\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5, verbose=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(document_term_matrix)\n",
        "\n",
        "# Print the best hyperparameters and corresponding score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Log Likelihood Score:\", grid_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# installing pyLDAvis to visualize the results of LDA model\n",
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "wVgfPyJoltpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing pyLDAvis module\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and display the graph\n",
        "lda_viz = gensimvis.prepare(lda, corpus, dictionary)\n",
        "lda_panel"
      ],
      "metadata": {
        "id": "ZQAEZflWsmRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# creating docterms dataframe\n",
        "docterms = lda_panel.token_table.sort_values(by = ['Freq'], ascending=False)\n",
        "\n",
        "# display docterms df\n",
        "docterms"
      ],
      "metadata": {
        "id": "tnwMHN4ntDhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create topics dataframe\n",
        "topicsdf = pd.DataFrame()\n",
        "\n",
        "\n",
        "# adding top 50 most relevant terms for each topic to the dataframe\n",
        "for i in range(1,6):\n",
        "  Topicdict ={ \"Topic\":i, \"Terms\":list(docterms[docterms['Topic']==i]['Term'].head(50))  }\n",
        "  topicsdf=topicsdf.append(Topicdict,ignore_index=True)\n",
        "topicsdf"
      ],
      "metadata": {
        "id": "yyCMakICtHhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOPIC 1 : POLITICS**"
      ],
      "metadata": {
        "id": "c1f8DEHvtQWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 1\n",
        "t1dict = {}\n",
        "for vals in docterms[docterms['Topic']==1].head(40).values:\n",
        "  t1dict[vals[2]] =vals[1]\n",
        "t1dict"
      ],
      "metadata": {
        "id": "C0kFSynRtPAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                min_font_size = 10).generate(' '.join(list(t1dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t1dict)\n",
        "\n",
        "\n",
        "\n",
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7UAzkoetOuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOPIC 2: TECH**\n"
      ],
      "metadata": {
        "id": "XPqFp7dttc8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 2\n",
        "t2dict = {}\n",
        "for vals in docterms[docterms['Topic']==2].head(40).values:\n",
        "  t2dict[vals[2]] =vals[1]\n",
        "t2dict"
      ],
      "metadata": {
        "id": "1KyFivDQtj_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                min_font_size = 10).generate(' '.join(list(t2dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t2dict)\n",
        "\n",
        "\n",
        "\n",
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cw8AsNOSto2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOPIC 3: SPORTS**"
      ],
      "metadata": {
        "id": "qHTwFhHJtrcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 3\n",
        "t3dict = {}\n",
        "for vals in docterms[docterms['Topic']==3].head(40).values:\n",
        "  t3dict[vals[2]] =vals[1]\n",
        "t3dict"
      ],
      "metadata": {
        "id": "_vhaXrUAtvw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                min_font_size = 10).generate(' '.join(list(t3dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t3dict)\n",
        "\n",
        "\n",
        "\n",
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HqJbEgBUt1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOPIC 4: ENTERTAINMENT**"
      ],
      "metadata": {
        "id": "nsI-FZZYt3VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 4\n",
        "t4dict = {}\n",
        "for vals in docterms[docterms['Topic']==4].head(40).values:\n",
        "  t4dict[vals[2]] =vals[1]\n",
        "t4dict"
      ],
      "metadata": {
        "id": "RYy3R4Yzt5iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                min_font_size = 10).generate(' '.join(list(t4dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t4dict)\n",
        "\n",
        "\n",
        "\n",
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8tFKJlawwpzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOPIC 5: BUSINESS**"
      ],
      "metadata": {
        "id": "6_oTWI1YwsRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 5\n",
        "t5dict = {}\n",
        "for vals in docterms[docterms['Topic']==5].head(40).values:\n",
        "  t5dict[vals[2]] =vals[1]\n",
        "t5dict"
      ],
      "metadata": {
        "id": "qwxlhUEAwvbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                min_font_size = 10).generate(' '.join(list(t5dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t5dict)\n",
        "\n",
        "\n",
        "\n",
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXQm-VNlwx8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**\n",
        "\n",
        "**EDA Conclusion :**\n",
        "1. Most of the content length lies between 0-5000 but some cases the length extends upto 25,000.\n",
        "2. The most articles are in the business and sports categories, followed by politics, entertainment, and technology.\n",
        "3. The term \"said\" appears the most in all article.\n",
        "\n"
      ],
      "metadata": {
        "id": "sO636f_Lw2fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Conclusion :**\n",
        "* Even though the LSA model did segregate the contents into five topic but the result was very bad, there were mixed up words in each topic.\n",
        "* So we won't be considering the LSA, and start of with LDA\n",
        "* Best log likelihood Score for the LDA model is -645320.7760682276\n",
        "* LDA model Perplexity on train data is 1601.6094311751326\n",
        "\n",
        "We successfully categorised the contents using the LDA method by analysing the common words related for each category.\n",
        "\n"
      ],
      "metadata": {
        "id": "WcL4Hgg9xFtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}